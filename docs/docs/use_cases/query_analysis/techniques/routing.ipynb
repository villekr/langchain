{
 "cells": [
  {
   "cell_type": "raw",
   "id": "a47da0d0-0927-4adb-93e6-99a434f732cf",
   "metadata": {},
   "source": [
    "---\n",
    "sidebar_position: 2\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2195672-0cab-4967-ba8a-c6544635547d",
   "metadata": {},
   "source": [
    "# Routing\n",
    "\n",
    "Sometimes we have multiple indexes for different domains, and for different questions we want to query different subsets of these indexes. For example, suppose we had one vector store index for all of the LangChain python documentation and one for all of the LangChain js documentation. Given a question about LangChain usage, we'd want to infer which language the the question was referring to and query the appropriate docs. **Query routing** is the process of classifying which index or subset of indexes a query should be performed on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4079b57-4369-49c9-b2ad-c809b5408d7e",
   "metadata": {},
   "source": [
    "## Setup\n",
    "#### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e168ef5c-e54e-49a6-8552-5502854a6f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain-core langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d66a45-a05c-4d22-b011-b1cdbdfc8f9c",
   "metadata": {},
   "source": [
    "#### Set environment variables\n",
    "\n",
    "We'll use OpenAI in this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40e2979e-a818-4b96-ac25-039336f94319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
    "\n",
    "# Optional, uncomment to trace runs with LangSmith. Sign up here: https://smith.langchain.com.\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b08c52-1ce9-4d8b-a779-cbe8efde51d1",
   "metadata": {},
   "source": [
    "## Routing with function calling models\n",
    "\n",
    "With function-calling models it's simple to use models for classification, which is what routing comes down to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "783c03c3-8c72-4f88-9cf4-5829ce6745d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bagatur/langchain/libs/core/langchain_core/_api/beta_decorator.py:86: LangChainBetaWarning: The function `with_structured_output` is in beta. It is actively being worked on, so the API may change.\n",
      "  warn_beta(\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
    "\n",
    "    datasource: Literal[\"python_docs\", \"js_docs\", \"golang_docs\"] = Field(\n",
    "        ...,\n",
    "        description=\"Given a user question choose which datasource would be most relevant for answering their question\",\n",
    "    )\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "structured_llm = llm.with_structured_output(RouteQuery)\n",
    "\n",
    "system = \"\"\"You are an expert at routing a user question to the appropriate data source.\n",
    "\n",
    "Based on the programming language the question is referring to, route it to the relevant data source.\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "router = prompt | structured_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61c682f1-9c46-4d7e-b909-5cfdabf41544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RouteQuery(datasource='python_docs')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"\"\"Why doesn't the following code work:\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\"human\", \"speak in {language}\"])\n",
    "prompt.invoke(\"french\")\n",
    "\"\"\"\n",
    "router.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3be4c9de-3b79-4f78-928c-0a65a0b87193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RouteQuery(datasource='js_docs')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"\"\"Why doesn't the following code work:\n",
    "\n",
    "\n",
    "import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n",
    "\n",
    "\n",
    "const chatPrompt = ChatPromptTemplate.fromMessages([\n",
    "  [\"human\", \"speak in {language}\"],\n",
    "]);\n",
    "\n",
    "const formattedChatPrompt = await chatPrompt.invoke({\n",
    "  input_language: \"french\"\n",
    "});\n",
    "\"\"\"\n",
    "router.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e01995-fa14-4f55-96e9-ae17d0d86e48",
   "metadata": {},
   "source": [
    "## Routing to multiple indexes\n",
    "\n",
    "If we may want to query multiple indexes we can do that, too, by updating our schema to accept a List of data sources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af62af17-4f90-4dbd-a8b4-dfff51f1db95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RouteQuery(datasources=['python_docs', 'js_docs'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
    "\n",
    "    datasources: List[Literal[\"python_docs\", \"js_docs\", \"golang_docs\"]] = Field(\n",
    "        ...,\n",
    "        description=\"Given a user question choose which datasources would be most relevant for answering their question\",\n",
    "    )\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "structured_llm = llm.with_structured_output(RouteQuery)\n",
    "router = prompt | structured_llm\n",
    "router.invoke(\n",
    "    {\n",
    "        \"question\": \"is there feature parity between the Python and JS implementations of OpenAI chat models\"\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poetry-venv-2",
   "language": "python",
   "name": "poetry-venv-2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
